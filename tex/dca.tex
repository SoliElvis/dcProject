\newcommand{\xs}{x^\nu}
\newcommand{\xss}{x^{\nu+1}}
\newcommand{\ys}{y^{\nu}}
\newcommand{\yss}{y^{\nu+1}}
\subsection{Intuition and Derivation of simplified DCA}
We now have the technical results necessary to discuss the
DC algorithm for general D.C. programs. Its aim is to 
generate sequences $x^\nu$ and $y^\nu$ which converge to a local
minimum of the functions $f:=g-h$ and $f^\dagger:= h^*-g^*$
respectively where $g,h\in\Gamma_0$\\ We define two sub programs:
\begin{align} 
 \widetilde S(x) \qquad &\inf\{h^*(y)-g^*(y) \ : \ y\in\partial h(x)\}\\ 
 \widetilde T(y) \qquad &\inf\{g(x)-h(x) \ : \ x\in\partial g^*(y)\}
 \label{fulldca}
\end{align}
Let $\widetilde {\mathcal S}(x)$ and $\widetilde{\mathcal T}(y)$ denote the
solution sets of $\widetilde S(x)$ and $\widetilde T(y)$ respectively. Then given
a point $x^o\in\ddom g$
the sequences are constructed as follows:
\begin{equation}
  \ys\in\widetilde{\mathcal S}(\xs); \quad \xss\in\widetilde{\mathcal T}(\ys)
\end{equation}
The main idea relies on a decomposition approach to the problem;
we use DC duality to solve the initial program and its dual
on subsets of the initial space using the subdifferential
operators to move between primal and dual. Eventhough the 
the subprograms are simpler than the initial one they are 
still non-convex. Their main function is to give 
insights as to how to construct sequences with the desired
behaviour. \\

Consider the program $\widetilde T(\ys)$ where $\ys$ is the solution
to some $\widetilde S(x^o)$. We have that $\ys\in\partial h(\xs)$ hence:
\begin{equation}
    h(\xs)\leq h(\xss) + \inp{y}{x-\xss} 
\end{equation}
This suggests a simplfication of the program $\widetilde T(\ys)$ by replacing
$h(\xs)$ by its affine minorization through $\ys$ whereby we get the
following program, which is convex: 
\begin{equation}
    \inf\{g(x) - [h(\xs)+\inp{\ys}{x-\xs}] \}
    \label{simDCApp}
\end{equation}
Dually we can also replace $g^*$ in the objective function
of $\widetilde S(\xss)$ by its affine minorant since we have that
$\xss \in \partial g^*(\ys)$:
\begin{equation}
    \inf\{h^*(y)-[g^*(\ys) + \inp{\xss}{y-\ys}]\}
    \label{simDCAdd}
\end{equation}
Note that we only needed the inclusion $\xss\in\partial g^*(\ys)$
and $\ys\in\partial h(\xs)$ to find a simplified versions of
the two initial programs.\\
\begin{comment}
Note that the simplified forms \eqref{simDCApp} and \eqref{simDCAdd}
are actually equivalent to the programs from \eqref{fulldca} if the functions 
$g^*$ and $h$ are actually essentially differentiable \autocite{tao2005dc}.
\end{comment}
\newpage
%
The programs \eqref{simDCApp} and \eqref{simDCAdd} can actually be simplified
further as the objective functions contain constants.
%
\begin{gather}
   \argmin\{h^*(y)-[g^*(\ys) + \inp{\xss}{y-\ys}]\}=
   \argmin\{h^*(y) -\inp{\xss}{y}\}\\
  \yss \in \argmin\{h^*(y) -\inp{\xss}{y}\} \iff \yss \in \partial h(\xss)
\end{gather}
Similarly
\begin{gather}
    \argmin\{g(x) - [h(\xs)+\inp{\ys}{x-\xs}] \}=
   \argmin\{g(x) -\inp{x}{\ys}\}\\
	\xss \in   \argmin\{g(x) -\inp{x}{\ys}\} \iff \xss \in \partial g^*(\ys)
\end{gather}

Hence all in all the simplified DCA boils down to following sequences:
\begin{equation}
	\ys\in\partial h(\xs) \qquad \xss\in\partial g^*(\ys)
\end{equation}
\newpage
\subsection{Analysis of Simplified DCA}
For conciseness we refer to the simplified DCA simply as the SDCA throughout
this section. It is defined by the following sequences:
\begin{equation}
	\ys\in\partial h(\xs) \qquad \xss\in\partial g^*(\ys)
\end{equation}
There are two main properties the SDCA satisfy:
\begin{enumerate}
    \item The sequences $f(\xs)$ and $f^\dagger (\ys)$ are decreasing
    \item Every limit point $\bar x$ and $\bar y$ of the sequences is a
	critical point of $f$ and $f^\dagger$ respectively
\end{enumerate}
In the following results the moduli of strong convexity can help
producing stronger estimates. Let $\lambda_F$ denote the modulus of
convexity for any strongly convex function $F$. Moreover we denote 
the increment vector $\xss-\xs$ as $\Delta \xs$.
We first present a proof of the first property as the following theorem:
\begin{theorem}
  Let the sequences $\xs$ and $\ys$ be generated by SDCA. Then we have  that
  \begin{align}
    f(\xss) &\leq f^\dagger (\ys) -\frac{\lambda_h}{2}\norm{\Delta \xs}^2 
      \leq f(\xs) - \frac{\lambda_g + \lambda_h}{2}\norm{\Delta \xs}^2\\
  \intertext{By duality we also have that:}
  f^\dagger(\yss) &\leq f (\xss) -\frac{\lambda_{h^*}}{2}\norm{\Delta \ys}^2 
      \leq f^\dagger(\ys) - \frac{\lambda_{g^*} + \lambda_{h^*}}{2}\norm{\Delta \ys}^2
  \end{align}
\begin{proof}
We drop sequence notation. Assuming we are at step $\nu$ 
we simply write $x$ and $y$ for $\xs$ and $\ys$ 
and $x^+$, $y^+$ for $\xss$ and $\yss$
\begin{align}
y\in\partial h(x)\qquad
  \nonumber \Rightarrow \qquad& h(x^+) \geq h(x) - \inp{y}{x-x^+} + \frac{\lambda_h}{2}\norm{\Delta x}^2\\
  \qquad \Rightarrow \qquad & (g-h)(x^+) \leq [g(x^+) - [h(x)-\inp{y}{x-x^+}]] - \frac{\lambda_h}{2}\norm{\Delta x}^2
  \label{descent:1}\\
  x^+\in\partial g^*(y) \qquad
  \nonumber \Rightarrow \qquad& g(x) \geq g(x^+) - \inp{y}{x^+-x} + \frac{\lambda_g}{2}\\
  \qquad \Rightarrow \qquad& (g-h)(x) \geq [g(x^+) - [h(x)-\inp{y}{x-x^+}]]+\frac{\lambda_g}{2}\norm{\Delta x}^2
  \label{descent:2}
\end{align}
Combining \eqref{descent:1} and \eqref{descent:2} we get 
\begin{equation}
  (g-h)(\xss) \leq [g(x^+) - [h(x)-\inp{y}{x-x^+}]]-\frac{\lambda_h}{2}\norm{\Delta \xs}^2
  \leq (g-h)(\xs) - \frac{\lambda_g + \lambda_h}{2}\norm{\Delta \xs}^2 \label{almostclaim}
\end{equation}
Now note that since $g,h\in \Gamma_o$ by assumption
\begin{equation}
  y\in\partial h(x) \iff \inp{x}{y}=h(x)+h^*(y) \qquad \text{and} \qquad 
  x^+\in\partial g^*(y) \iff \inp{x^+}{y}=g(x^+)+g^*(y)
\end{equation}
Combining the above we get that 
\begin{equation}
  g(x^+)-[h(x)-\inp{y}{x-x^+}]=(h^*-g^*)(y)
\end{equation}
Substituting the preceding expression into \eqref{almostclaim} we get the first inequality 
of the claim. The second one is obtained in exactly the same way by symmetry.
\end{proof}
\end{theorem}
\newpage

