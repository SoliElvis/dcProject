We define a couple of key concepts. 
First for exposition we use the following function notation throughout:
\begin{equation*}
	f: \mathbb R^n \longrightarrow \overline{\mathbb R} 
\end{equation*}
We also use Rockafellar's notation \autocite{rockafellar2009variational} for 
sequences of real variables : $x^{\nu}$ where $\nu$ is always understood 
to be a natural number used as an index and not an exponent.
\begin{definition}
A function's \emph{domain} is the subset of the initial set 
of the function where it doesn't attain infinity. Rockafellar
and other's sometimes refer to this set as the \emph{effective} domain
to emphasize the difference with the naive set theory definition. However
we will always refer to the domain in the first sense and will dispense from
using the adjective "effective". Formally:
\begin{equation*}
	\ddom f := \{x\in\mathbb R^n \ |\ f(x)< \infty\}
\end{equation*}
\end{definition}
%
\begin{definition}
	A convex function is said to be \emph{proper} if it has non empty domain
	and $f(x)>-\infty$ for all $x\in\mathbb R^n$. The set of all such
	functions is denoted $\Gamma$.
\end{definition}

\noindent In optimisation smoothness assumptions often break-down. We thus need
a different machinery than the one obtained from classical calculus. We
introduce some of those notions now. 

\begin{definition}
We define present the definition of the lower limit of a function $f$ at some
$\bar x\in\mathbb R^n$ as it is written by Rockafellar and Wets
\autocite{rockafellar2009variational}
\begin{align*}
	\liminf_{x\rightarrow \bar x} f(x) :&= \lim_{\delta \searrow 0}
	\left[\inf_{x\in B(\bar x,\delta)}f(x)\right]\\[2ex]
		&=\sup_{\delta >0}\left[\inf_{x\in B(\bar x,\delta)}f(x)\right]
		\quad = \sup_{V\in \mathcal{N}(\bar x)}\left[\inf_{x\in
		V}f(x)\right]
\end{align*}
\end{definition}
\begin{prop}
	\autocite{rockafellar2009variational}\\
The above definition of lower limit of a function is equivalent to
\begin{equation*}
	\liminf_{x\rightarrow\bar x}f(x) = \min\{\alpha\in\overline{\mathbb R}\ 
		| \ \exists x^{\nu} \rightarrow \bar x \ \text{with}
	\ f(x^{\nu})\rightarrow \alpha\} 
\end{equation*} We use min on the rhs as the definition presumes that the value
is actually attained.
\end{prop}
%
\begin{definition}
The concept of lower-limit allows us to talk about semicontinuity. 
A function $f$ is said to be \emph{lower semicontinuous} (lsc) at $\bar x$  if
\begin{equation*}
	\liminf_{x\rightarrow\bar x}f(x) \geq f(\bar x) 
	\qquad \text{or equivalently} \qquad 
	\liminf_{x\rightarrow\bar x}f(x)=f(\bar x)
\end{equation*}
Where the equivalence comes from the fact that the reverse inequality always
holds by definition of $\liminf$
\end{definition}
%
\clearpage
\begin{definition}
Recalling the definition of proper convex functions; when $f\in\Gamma$ and is
lsc we say that $f\in \Gamma_0$ which is set the of all such functions.
\end{definition} Those sets being of major importance we restate, more
formally:
\begin{equation*}
	\Gamma:= \{f:\mathbb R^n \longrightarrow \mathbb \overline{\mathbb R} \
	| \ f \ \text{proper and convex}\}
	\qquad \Gamma_0 := 
	\{f:\mathbb R^n \longrightarrow \mathbb \overline{\mathbb R} \
	| \ f \ \text{proper , lsc and convex}\}
\end{equation*}
%
\begin{definition}
	Let $f: \mathbb R^n \longrightarrow \overline{\mathbb R}$ be convex and
	$x\in\mathbb R^n$. Then $g\in\mathbb R^n$ is called a subgradient of
	$f$ at $\bar x$ if 
	\begin{equation*}
		f(x) \geq f(\bar x) +\langle g,x-\bar x\rangle \quad 
		\forall x\in\mathbb R^n 
	\end{equation*}
Moreover the set of all such subgradients of $f$ at $\bar x$ is callled the subdifferential
of $f$ at $\bar x$ and we denote it as $\partial f(\bar x)$.
\end{definition}
%
\begin{definition}
The Fenchel conjugate of a function $f$, in the convex analysis setting often
just referred to as the conjugate of a function, is an extended real valued
functione defined as:
\begin{equation*}
	f^*(y) := \sup_{x\in\mathbb R^n}\{\langle x,y\rangle - f(x)\}
\end{equation*}
The mapping $f\mapsto f^*$ is called the Legendre-Fenchel transform.
It is sometimes just refered to as the Legendre transform in physics which 
doesn't do justice to Fenchel's important work in the non-differentiable
and variational case.
\end{definition}
\noindent By definition we always have that
\begin{equation*} f(x)+f^*(y)\geq\langle x,y\rangle \quad \forall\ x,y\in 
\mathbb R^n\end{equation*}
This relationship is known as the \emph{Fenchel-Young Inequality}

\begin{lemma}\label{fenchelDualLemma}
	Let $f\in \Gamma$, then $y\in \partial f(x) \iff 
	f(x) + f^*(y) = \langle x,y \rangle $
\end{lemma}
\begin{proof}
Since we already have the Fenchel-Young inequality we only need the reverse:
\begin{align*}
	y \in \partial f(x) \overset{\mathrm{def}}{\iff}&
	f(z)\geq f(x) +\langle y,z-x\rangle 
	\qquad \forall \, z\in\mathbb R^n\\
	\iff& f(x) + \sup_z\{\langle
	y,z\rangle -f(z)\}
	\leq \langle x,y \rangle \\
	\iff& f(x)+ f^*(y) \leq \langle x,y\rangle
\end{align*}
\end{proof}
\newpage
The  \emph{$\epsilon$-subdifferential} of a function.  is a tool which has been
mostly developped by J.B. Hiriart-Urruty in the second of his two volumes on
convex analysis \autocite[92]{hiriart1993convex} 
\begin{definition} 
	Given $x\in\ddom f$ the vector $s\in \mathbb R$ is called
	an $\epsilon-subgradient$ of $f$ at $x$, written $s\in
	\partial_\epsilon f(x)$ when the following holds
	\begin{equation*}
		f(y) \geq f(x) + \langle s,y-x\rangle -\epsilon 
		\quad \forall \ y \in \mathbb R^n \quad
	\end{equation*}
The set of all $\epsilon$-subgradients of a function at some point
$x\in\mathbb R^n$ is called the $\epsilon$-subdifferential of $f$ at $x$.
Moreover it is clear from the definition that
$\partial f(x) = \bigcap_{\epsilon>0}\partial_{\epsilon}f(x)$
\end{definition}

We have an equivalent to the Fenchel-Young inequality for the $\epsilon-$
subdifferentials. We present it as a lemma here:
\begin{lemma}
	Let $f\in\Gamma$ and $x^o \in\ddom f$ then we have that
	$y \in\partial_\epsilon f(x^o)$ if and only if 
	$f(x^o)+f^*(y) \leq \langle x^o,y\rangle + \epsilon$ 
	\label{FYepsilon}
\end{lemma}
\begin{proof}
	\begin{align*}
		y\in\partial_\epsilon f(x^o) &\iff 
		f(x) \geq f(x^o) + \inp{y}{x-x^o} - \epsilon
		\qquad x\in\mathbb R^n \\
		&\iff \epsilon + \inp{y}{x^o} \geq f(x^o) 
		+\underbrace{\sup_x \{\inp{x}{y} - f(x)\}}_{f^*(y)}
		&\iff \epsilon + \inp{y}{x^o} \geq f(x^o) + f^*(y)
\end{align*}

\end{proof}
