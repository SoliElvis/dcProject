We define a couple of key concepts. 
First for exposition we use the following function notation throughout:
\begin{equation*}
	f: \mathbb R^n \longrightarrow \overline{\mathbb R} 
\end{equation*}
We also use Rockafellar's notation \autocite{rockafellar2009variational} for 
sequences of real variables : $x^{\nu}$ where $\nu$ is always understood 
to be a natural number used as an index and not an exponent.
\begin{definition}
A function's \emph{domain} is the subset of the initial set 
of the function where it doesn't attain infinity. Rockafellar
and other's sometimes refer to this set as the \emph{effective} domain
to emphasize the difference with the naive set theory definition. However
we will always refer to the domain in the first sense and will dispense from
using the adjective "effective". Formally:
\begin{equation*}
	\ddom f := \{x\in\mathbb R^n \ |\ f(x)< \infty\}
\end{equation*}
\end{definition}
%
\begin{definition}
	A convex function is said to be \emph{proper} if it has non empty domain
	and $f(x)>-\infty$ for all $x\in\mathbb R^n$. The set of all such
	functions is denoted $\Gamma$.
\end{definition}

\noindent In optimisation smoothness assumptions often break-down. We thus need
a different machinery than the one obtained from classical calculus. We
introduce some of those notions now. 

\begin{definition}
We define present the definition of the lower limit of a function $f$ at some
$\bar x\in\mathbb R^n$ as it is written by Rockafellar and Wets
\autocite{rockafellar2009variational}
\begin{align*}
	\liminf_{x\rightarrow \bar x} f(x) :&= \lim_{\delta \searrow 0}
	\left[\inf_{x\in B(\bar x,\delta)}f(x)\right]\\[2ex]
		&=\sup_{\delta >0}\left[\inf_{x\in B(\bar x,\delta)}f(x)\right]
		\quad = \sup_{V\in \mathcal{N}(\bar x)}\left[\inf_{x\in
		V}f(x)\right]
\end{align*}
\end{definition}
\begin{prop}
	\autocite{rockafellar2009variational}\\
The above definition of lower limit of a function is equivalent to
\begin{equation*}
	\liminf_{x\rightarrow\bar x}f(x) = \min\{\alpha\in\overline{\mathbb R}\ 
		| \ \exists x^{\nu} \rightarrow \bar x \ \text{with}
	\ f(x^{\nu})\rightarrow \alpha\} 
\end{equation*} We use min on the rhs as the definition presumes that the value
is actually attained.
\end{prop}
%
\begin{definition}
The concept of lower-limit allows us to talk about semicontinuity. 
A function $f$ is said to be \emph{lower semicontinuous} (lsc) at $\bar x$  if
\begin{equation*}
	\liminf_{x\rightarrow\bar x}f(x) \geq f(\bar x) 
	\qquad \text{or equivalently} \qquad 
	\liminf_{x\rightarrow\bar x}f(x)=f(\bar x)
\end{equation*}
Where the equivalence comes from the fact that the reverse inequality always
holds by definition of $\liminf$
\end{definition}
%
\begin{definition}
Recalling the definition of proper convex functions; when $f\in\Gamma$ and is
lsc we say that $f\in \Gamma_0$ which is set the of all such functions.
\end{definition} Those sets being of major importance we restate, more
formally:
\begin{equation*}
	\Gamma:= \{f:\mathbb R^n \longrightarrow \mathbb \overline{\mathbb R} \
	| \ f \ \text{proper and convex}\}
	\qquad \Gamma_0 := 
	\{f:\mathbb R^n \longrightarrow \mathbb \overline{\mathbb R} \
	| \ f \ \text{proper , lsc and convex}\}
\end{equation*}
%
\begin{definition}
	Let $f: \mathbb R^n \longrightarrow \overline{\mathbb R}$ be convex and
	$x\in\mathbb R^n$. Then $g\in\mathbb R^n$ is called a subgradient of
	$f$ at $\bar x$ if 
	\begin{equation*}
		f(x) \geq f(\bar x) +\langle g,x-\bar x\rangle \quad 
		\forall x\in\mathbb R^n 
	\end{equation*}
	Moreover the set of all such subgradients of $f$ at $\bar x$ is callled
	the \emph{subdifferential} of $f$ at $\bar x$ and we denote it as
	$\partial f(\bar x)$.
\end{definition}
%
The  \emph{$\epsilon$-subgradient} of a function is a generalization 
of the subgradient and  has been mostly developped by J.B. Hiriart-Urruty
in the second of his two volumes on convex analysis
\autocite[92]{hiriart1993convex} 
\begin{definition} 
	Given $x\in\ddom f$ the vector $s\in \mathbb R$ is called
	an $\epsilon-subgradient$ of $f$ at $x$, written $s\in
	\partial_\epsilon f(x)$ when the following holds
	\begin{equation*}
		f(y) \geq f(x) + \langle s,y-x\rangle -\epsilon 
		\quad \forall \ y \in \mathbb R^n \quad
	\end{equation*}
The set of all $\epsilon$-subgradients of a function at some point
$x\in\mathbb R^n$ is called the $\epsilon$-subdifferential of $f$ at $x$.
Moreover it is clear from the definition that
$\partial f(x) = \bigcap_{\epsilon>0}\partial_{\epsilon}f(x)$
\end{definition}
%
\begin{definition}
The Fenchel conjugate of a function $f$, in the convex analysis setting often
just referred to as the conjugate of a function, is an extended real valued
functione defined as:
\begin{equation*}
	f^*(y) := \sup_{x\in\mathbb R^n}\{\langle x,y\rangle - f(x)\}
\end{equation*}
The mapping $f\mapsto f^*$ is called the Legendre-Fenchel transform.
It is sometimes just refered to as the Legendre transform in physics which 
doesn't do justice to Fenchel's important work in the non-differentiable
and variational case.
\end{definition}
\noindent By definition we always have that
\begin{equation*} f(x)+f^*(y)\geq\langle x,y\rangle \quad \forall\ x,y\in 
\mathbb R^n\end{equation*}
This relationship is known as the \emph{Fenchel-Young Inequality}


\begin{lemma}
	Let $f\in\Gamma$ and $x^o \in\ddom f$ then we have that
	$y \in\partial_\epsilon f(x^o)$ if and only if 
	$f(x^o)+f^*(y) \leq \langle x^o,y\rangle + \epsilon$ 
	\label{FYepsilon}
\end{lemma}
\begin{proof}
	\begin{align*}
		y\in\partial_\epsilon f(x^o) &\iff 
		f(x) \geq f(x^o) + \inp{y}{x-x^o} - \epsilon
		\qquad x\in\mathbb R^n \\
		&\iff \epsilon + \inp{y}{x^o} \geq f(x^o) 
		+\underbrace{\sup_x \{\inp{x}{y} - f(x)\}}_{f^*(y)}
		&\iff \epsilon + \inp{y}{x^o} \geq f(x^o) + f^*(y)
\end{align*}
\end{proof}
\begin{lemma}\label{fenchelDualLemma}
	Let $f\in \Gamma$, then $y\in \partial f(x) \iff 
	f(x) + f^*(y) = \langle x,y \rangle $
\end{lemma}
\begin{proof}
    Since we already have that $f(x)+f^*(y)\geq\langle x,y\rangle \quad \forall\ x,y\in\mathbb R^n$ (Fenchel-Young)
    we only need the reverser inequality.\\
Let $y \in \partial f(x)$ 
\begin{align*}
    y \in \partial f(x) &\iff y \in \bigcap_{\epsilon>0}\partial_{\epsilon}f(x)\\
    \text{(By the preceding lemma)} \qquad 
    &\iff f(x) + f^*(y) \leq \inp{x}{y} + \epsilon \qquad \forall \ \epsilon > 0 \\
    &\iff f(x) + f^*(y) \leq \inp{x}{y} 
\end{align*}
\end{proof}
\begin{lemma}\label{lemma:neighToGlobal}
  Let $f$ be a convex function. Then provided that the following
  holds:
  \begin{equation}
    f(x) \geq f(\bar x) + \inp{g}{x-\bar x} 
  \end{equation}
  for all $x$ in some some neighborhood of $\bar x$ then it actually
  hold for all $x \in \mathbb R^n$ and $g$ is thus a subgradient of
  f at $\bar x$
  \begin{proof}
    Let $N(\bar x)$ designate the neighborhood of $\bar x$ for which
    the inequality holds. Let $\lambda >0$ be small enough 
    so that $(\lambda x+ (1-\lambda)\bar x) \in N(\bar x)$
    By convexity we have :
    \begin{align}
%
      \lambda f(x) + (1-\lambda)f(\bar x) &\geq
      f(\lambda x + (1-\lambda)\bar x) \qquad \forall \ x \in \mathbb R^n  \\
      &\geq f(\bar x) + \inp{g}{[\lambda x + (1-\lambda)\bar x)] - \bar x} 
      \qquad \forall \ x \in \mathbb R^n 
%
    \end{align}
  Letting $\lambda$ go to zero gives us the desired inequality.
  \end{proof}
\end{lemma}
\clearpage

\begin{definition}
        A convex function is said to be  strongly convex with modulus
	$\mu>0$ if 
        \begin{equation}
          f(x) - \frac{\mu}{2}\norm{x}^2
        \end{equation}
        defines a convex function.\\
\end{definition}

\begin{lemma}
    If a function $f$ is strongly convex with modulus $\mu>0$  then 
  $y\in \partial f(\bar x)$ if and only if:
  \begin{equation}
    f(\bar x) \leq f(x) + \inp{y}{x-\bar x} - \frac{\mu}{2}\norm{x-\bar x}^2
  \end{equation}
  \begin{proof} 
      Let $y \in \partial f(\bar x)$
      Let $f$ be strongly convex with modulus $\mu>0$, i.e. $g(x) = f(x) -
      \frac{\mu}{2}\norm{x}^2$ defines a convex function. Then by definition of convexity 
      we have :
      \begin{align}
	  f(\lambda x + (1-\lambda)y) &\leq \lambda f(x) + (1-\lambda)f(y) 
	  + \frac{\sigma}{2}(\norm{\lambda x + (1-\lambda y)}^2 - \norm{x}^2 -
	  (1-\lambda)\norm{y}^2)\\
%
	  &\leq \lambda f(x) + (1-\lambda)f(y) -
	  \frac{\sigma}{2}\lambda(1-\lambda)\norm{x-y}^2 \label{strong:alternate}
      \end{align}
      \underline{Remark:} The last inequality is often taken to be the definition 
      of strong convexity where the modulus of strong convexity is $\mu$.
      
\end{proof}
\end{lemma}








